{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import xgboost as xgb\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.metrics import DistanceMetric\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics import silhouette_score\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round2의 weather 예측값 전처리(Expert model의 input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_real = pd.read_csv(\"./weather_actual.csv\")\n",
    "weather_index = weather_real['time']\n",
    "\n",
    "weather = pd.read_csv(\"./weather_forecast.csv\")\n",
    "weather1 = weather[weather['round']==1].copy()\n",
    "weather2 = weather[weather['round']==2].copy()\n",
    "weather1_index = weather1.reset_index(drop=True)['time']\n",
    "weather2_index = weather2.reset_index(drop=True)['time']\n",
    "weather1_val = pd.read_csv(\"./weather_forecast_round1_val.csv\")\n",
    "weather2_val = pd.read_csv(\"./weather_forecast_round2_val.csv\")\n",
    "\n",
    "pred = pd.read_csv(\"./pred.csv\")\n",
    "pred1 = pred[pred['round']==1].copy()\n",
    "pred2 = pred[pred['round']==2].copy()\n",
    "pred1_index = pred1.reset_index(drop=True)['time']\n",
    "pred2_index = pred2.reset_index(drop=True)['time']\n",
    "pred1_val = pd.read_csv(\"./gen_forecast_round1_val.csv\")\n",
    "pred2_val = pd.read_csv(\"./gen_forecast_round2_val.csv\")\n",
    "\n",
    "gen = pd.read_csv(\"./gens.csv\")\n",
    "gen_val = pd.read_csv(\"./add_gens.csv\")\n",
    "gen_index = gen['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array(weather2_index.tolist())\n",
    "B = np.array(pred2_index.tolist())\n",
    "C = np.array(gen_index.tolist())\n",
    "A1 = np.array(weather1_index.tolist())\n",
    "B1 = np.array(pred1_index.tolist())\n",
    "total_index = np.intersect1d(np.intersect1d(A,B),C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(np.setdiff1d(A,total_index)):\n",
    "    weather2.drop([weather2[weather2['time']==i].index.item()],inplace=True)\n",
    "\n",
    "for i in list(np.setdiff1d(B,total_index)):\n",
    "    pred2.drop(pred2[pred2['time']==i].transpose().columns,inplace=True)\n",
    "\n",
    "for i in list(np.setdiff1d(A1,total_index)):\n",
    "    weather1.drop([weather1[weather1['time']==i].index.item()],inplace=True)\n",
    "\n",
    "for i in list(np.setdiff1d(B1,total_index)):\n",
    "    pred1.drop(pred1[pred1['time']==i].transpose().columns,inplace=True)\n",
    "\n",
    "for i in list(np.setdiff1d(C,total_index)):\n",
    "    gen.drop([gen[gen['time']==i].index.item()],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real_weather & pred_weather 오차\n",
    "weather1 = weather1.drop(['round','time'],axis=1)\n",
    "weather2 = weather2.drop(['round','time'],axis=1)\n",
    "weather1_val = weather1_val.drop(['round','time'],axis=1)\n",
    "weather2_val = weather2_val.drop(['round','time'],axis=1)\n",
    "\n",
    "ss1 = MinMaxScaler()\n",
    "ss2 = MinMaxScaler()\n",
    "\n",
    "ss_weather1 = pd.DataFrame(ss1.fit_transform(weather1))\n",
    "ss_weather2 = pd.DataFrame(ss2.fit_transform(weather2))\n",
    "ss_weather1_val = pd.DataFrame(ss1.transform(weather1_val))\n",
    "ss_weather2_val = pd.DataFrame(ss2.transform(weather2_val))\n",
    "ss_weather1, ss_weather2\n",
    "\n",
    "# ss_weather = pd.concat([ss_weather, (weather-weather_real)/(weather_real.min()-weather_real.max())],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round2의 Expert model 결과값 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = pred1.drop(['round', 'time'],axis=1)\n",
    "pred2 = pred2.drop(['round', 'time'],axis=1)\n",
    "pred1_val = pred1_val.drop(['round', 'time'],axis=1)\n",
    "pred2_val = pred2_val.drop(['round', 'time'],axis=1)\n",
    "for i in range(5):\n",
    "    exec(f\"pred{i}_1 = pred1[pred1['model_id']==i].drop(['model_id'], axis=1).reset_index(drop=True)\")\n",
    "    exec(f\"pred{i}_1.columns = ['model{i}']\")\n",
    "    exec(f\"pred{i}_2 = pred2[pred2['model_id']==i].drop(['model_id'], axis=1).reset_index(drop=True)\")\n",
    "    exec(f\"pred{i}_2.columns = ['model{i}']\")\n",
    "\n",
    "    exec(f\"pred{i}_1_val = pred1_val[pred1_val['model_id']==i].drop(['model_id'], axis=1).reset_index(drop=True)\")\n",
    "    exec(f\"pred{i}_1_val.columns = ['model{i}']\")\n",
    "    exec(f\"pred{i}_2_val = pred2_val[pred2_val['model_id']==i].drop(['model_id'], axis=1).reset_index(drop=True)\")\n",
    "    exec(f\"pred{i}_2_val.columns = ['model{i}']\")\n",
    "    \n",
    "gen_pred1 = pd.concat([eval(f'pred{i}_1') for i in range(5)], axis=1)\n",
    "gen_pred2 = pd.concat([eval(f'pred{i}_2') for i in range(5)], axis=1)\n",
    "gen_pred1_val = pd.concat([eval(f'pred{i}_1_val') for i in range(5)], axis=1)\n",
    "gen_pred2_val = pd.concat([eval(f'pred{i}_2_val') for i in range(5)], axis=1)\n",
    "\n",
    "gen_pred1, gen_pred2, gen_pred1_val, gen_pred2_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실제 gen 값 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = gen.drop(['time'], axis=1)\n",
    "gen_val = gen_val.drop(['time'], axis=1)\n",
    "gen, gen_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습/검증 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Validation Shape는 11월 12일 00:10 기준 24x22 = 528 행이어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_weather_dataset1 = torch.tensor(np.array(ss_weather1)).type(torch.float32)\n",
    "gen_pred_dataset1 = torch.tensor(np.array(gen_pred1)).type(torch.float32)\n",
    "ss_weather_dataset2 = torch.tensor(np.array(ss_weather2)).type(torch.float32)\n",
    "gen_pred_dataset2 = torch.tensor(np.array(gen_pred2)).type(torch.float32)\n",
    "gen_dataset = torch.tensor(np.array(gen)).type(torch.float32)\n",
    "\n",
    "ss_weather_dataset1_val = torch.tensor(np.array(ss_weather1_val)).type(torch.float32)\n",
    "gen_pred_dataset1_val = torch.tensor(np.array(gen_pred1_val)).type(torch.float32)\n",
    "ss_weather_dataset2_val = torch.tensor(np.array(ss_weather2_val)).type(torch.float32)\n",
    "gen_pred_dataset2_val = torch.tensor(np.array(gen_pred2_val)).type(torch.float32)\n",
    "gen_dataset_val = torch.tensor(np.array(gen_val)).type(torch.float32)\n",
    "\n",
    "ss_weather_dataset1.shape, gen_pred_dataset1.shape, ss_weather_dataset2.shape, gen_pred_dataset2.shape, gen_dataset.shape, ss_weather_dataset1_val.shape, gen_pred_dataset1_val.shape, ss_weather_dataset2_val.shape, gen_pred_dataset2_val.shape, gen_dataset_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather_pred 13 & gen_pred 5 & gen 1 \n",
    "total_dataset1 = torch.cat([ss_weather_dataset1, gen_pred_dataset1, gen_dataset], dim=1)\n",
    "total_dataset2 = torch.cat([ss_weather_dataset2, gen_pred_dataset2, gen_dataset], dim=1)\n",
    "total_dataset1_val = torch.cat([ss_weather_dataset1_val, gen_pred_dataset1_val, gen_dataset_val], dim=1)\n",
    "total_dataset2_val = torch.cat([ss_weather_dataset2_val, gen_pred_dataset2_val, gen_dataset_val], dim=1)\n",
    "\n",
    "total_dataset1.shape, total_dataset2.shape, total_dataset1_val.shape, total_dataset2_val.shape,      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간 고려 -> Train : Val = 앞 90% : 뒤 10%\n",
    "train_dataset1 = total_dataset1\n",
    "val_dataset1 = total_dataset1_val\n",
    "train_dataset2 = total_dataset2\n",
    "val_dataset2 = total_dataset2_val\n",
    "\n",
    "train_dataset1.shape, val_dataset1.shape, train_dataset2.shape, val_dataset2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch dataset(input_X : pred_weather, infer_X : pred_gen, true_Y : gen)\n",
    "def make_batch_dataset(train_dataset, random_idx):\n",
    "    batch_dataset = train_dataset[random_idx]\n",
    "    batch_input_X = batch_dataset[:,:18]\n",
    "    batch_infer_X = batch_dataset[:,13:18]\n",
    "    batch_true_Y = batch_dataset[:,[18]]\n",
    "    return batch_input_X, batch_infer_X, batch_true_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_validation_dataset(val_dataset):\n",
    "    batch_dataset = val_dataset\n",
    "    batch_input_X = batch_dataset[:,:18]\n",
    "    batch_infer_X = batch_dataset[:,13:18]\n",
    "    batch_true_Y = batch_dataset[:,[18]]\n",
    "    return batch_input_X, batch_infer_X, batch_true_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loss10(batch_true_Y, batch_pred_Y1):\n",
    "    # Utilization preprocess\n",
    "    batch_true_Y[batch_true_Y < 9.9] = 0\n",
    "\n",
    "    error_abs = torch.abs(batch_pred_Y1- batch_true_Y)\n",
    "    error_type1 = torch.sqrt(error_abs[((error_abs/99*100)>6) & ((error_abs/99*100)<=8) & (batch_true_Y > 0)])\n",
    "    error_type2 = torch.sqrt(error_abs[((error_abs/99*100)>8) & (batch_true_Y > 0)])\n",
    "    cost_type1 = batch_true_Y[((error_abs/99*100)>6) & ((error_abs/99*100)<=8) & (batch_true_Y > 0)]\n",
    "    cost_type2 = batch_true_Y[((error_abs/99*100)>8) & (batch_true_Y > 0)] * 4\n",
    "    error_type1_num = len(cost_type1)\n",
    "    error_type2_num = len(cost_type2)\n",
    "    batch_size = len(batch_true_Y[batch_true_Y > 0])\n",
    "\n",
    "    # loss\n",
    "    if (error_type1_num != 0) and (error_type2_num != 0):\n",
    "        loss = (torch.sum(error_type1 * cost_type1) + torch.sum(error_type2 * cost_type2)) / (batch_size)\n",
    "    elif (error_type1_num != 0):\n",
    "        loss = torch.sum(error_type1 * cost_type1) / (batch_size)\n",
    "    elif (error_type2_num != 0):\n",
    "        loss = torch.sum(error_type2 * cost_type2) / (batch_size)\n",
    "    else:\n",
    "        loss = torch.tensor(0.)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation10(val_batch_true_Y, val_batch_pred_Y1):\n",
    "    val_true_Y = np.array(val_batch_true_Y.detach())\n",
    "    val_pred_Y1 = np.array(val_batch_pred_Y1.detach())\n",
    "\n",
    "    val_true_Y[val_true_Y < 9.9] = 0\n",
    "\n",
    "    error_abs = np.abs(val_pred_Y1 - val_true_Y)\n",
    "    cost_type1 = val_true_Y[((error_abs/99*100)>6) & ((error_abs/99*100)<=8) & (val_true_Y > 0)]\n",
    "    cost_type2 = val_true_Y[((error_abs/99*100)>8) & (val_true_Y > 0)] * 4\n",
    "    full_incentive = np.sum(val_true_Y[(val_true_Y > 0)] * 4)\n",
    "    error_type1_num = len(cost_type1)\n",
    "    error_type2_num = len(cost_type2)\n",
    "\n",
    "    # loss\n",
    "    if (error_type1_num != 0) and (error_type2_num != 0):\n",
    "        loss = (np.sum(cost_type1) + np.sum(cost_type2))\n",
    "    elif (error_type1_num != 0):\n",
    "        loss = np.sum(cost_type1)\n",
    "    elif (error_type2_num != 0):\n",
    "        loss = np.sum(cost_type2)\n",
    "    else:\n",
    "        loss = 0.\n",
    "\n",
    "    \n",
    "    return loss, (1-loss/full_incentive)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(val_batch_true_Y, val_batch_pred_Y1, val_batch_pred_Y2):\n",
    "    val_true_Y = np.array(val_batch_true_Y.detach())\n",
    "    val_pred_Y1 = np.array(val_batch_pred_Y1.detach())\n",
    "    val_pred_Y2 = np.array(val_batch_pred_Y2.detach())\n",
    "\n",
    "    val_true_Y[val_true_Y < 9.9] = 0\n",
    "\n",
    "    error_abs = np.abs(np.round((val_pred_Y1+val_pred_Y2)/2) - val_true_Y)\n",
    "    cost_type1 = val_true_Y[((error_abs/99*100)>6) & ((error_abs/99*100)<=8) & (val_true_Y > 0)]\n",
    "    cost_type2 = val_true_Y[((error_abs/99*100)>8) & (val_true_Y > 0)] * 4\n",
    "    full_incentive = np.sum(val_true_Y[(val_true_Y > 0)] * 4)\n",
    "    error_type1_num = len(cost_type1)\n",
    "    error_type2_num = len(cost_type2)\n",
    "\n",
    "    # loss\n",
    "    if (error_type1_num != 0) and (error_type2_num != 0):\n",
    "        loss = (np.sum(cost_type1) + np.sum(cost_type2))\n",
    "    elif (error_type1_num != 0):\n",
    "        loss = np.sum(cost_type1)\n",
    "    elif (error_type2_num != 0):\n",
    "        loss = np.sum(cost_type2)\n",
    "    else:\n",
    "        loss = 0.\n",
    "\n",
    "    \n",
    "    return loss, (1-loss/full_incentive)*100, full_incentive-loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble_model1(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Ensemble_model1, self).__init__()\n",
    "        self.fe = nn.Linear(18, 128)\n",
    "        self.hidden1 = nn.Linear(128,128)\n",
    "        self.hidden2 = nn.Linear(128,128)\n",
    "        self.hidden3 = nn.Linear(128,128)\n",
    "        self.hidden4 = nn.Linear(128,128)\n",
    "        self.latent = nn.Linear(128, 5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, x1):\n",
    "        batch_size = len(x)\n",
    "        x = self.fe(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.hidden3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.hidden4(x)\n",
    "        x = self.relu(x)\n",
    "        f = self.latent(x)\n",
    "        x = torch.sum(f*x1, 1).reshape(batch_size, -1)\n",
    "        x = self.relu(x)\n",
    "        return (x, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble_model2(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Ensemble_model2, self).__init__()\n",
    "        self.fe = nn.Linear(18, 64)\n",
    "        self.hidden1 = nn.Linear(64,64)\n",
    "        self.hidden2 = nn.Linear(64,64)\n",
    "        self.hidden3 = nn.Linear(64,64)\n",
    "        self.hidden4 = nn.Linear(64,64)\n",
    "        self.hidden5 = nn.Linear(64,64)\n",
    "        self.latent = nn.Linear(64, 5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, x1):\n",
    "        batch_size = len(x)\n",
    "        x = self.fe(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.hidden3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.hidden4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.hidden5(x)\n",
    "        x = self.relu(x)\n",
    "        f = self.latent(x)\n",
    "        x = torch.sum(f*x1, 1).reshape(batch_size, -1)\n",
    "        x = self.relu(x)\n",
    "        return (x, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 아래의 train_date는 입찰날짜+1 or 예측 대상 날짜를 입력하면 됨\n",
    "#### 11월 12일 00:10 기준 2023-11-13 임!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### 반드시 train_date 수정할 것!!!!!!!! ##########################\n",
    "train_date = \"2023-11-13\"\n",
    "####################### 반드시 train_date 수정할 것!!!!!!!! ##########################\n",
    "\n",
    "\n",
    "model1 = Ensemble_model1()\n",
    "model2 = Ensemble_model2()\n",
    "\n",
    "import copy\n",
    "epoch = 1000000\n",
    "lr = 1e-5\n",
    "optimizer1 = optim.Adam(model1.parameters(), lr=lr)\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=lr)\n",
    "batch_size1 = 128\n",
    "batch_size2 = 128\n",
    "best_val_loss = 10000000000\n",
    "best_accuracy = 0\n",
    "best_model1 = model1\n",
    "best_model2 = model2\n",
    "local_best_model1 = model1\n",
    "local_best_model2 = model2\n",
    "local_val_loss1 = 1000000000\n",
    "local_val_loss2 = 1000000000\n",
    "\n",
    "\n",
    "val_input_X1, val_infer_X1, val_true_Y = make_validation_dataset(val_dataset1)\n",
    "val_input_X2, val_infer_X2, val_true_Y = make_validation_dataset(val_dataset2)\n",
    "\n",
    "for epoch_idx in range(epoch):\n",
    "    \n",
    "    random_idx = random.sample(range(len(train_dataset1)), k=batch_size1)\n",
    "    batch_input_X1, batch_infer_X1, batch_true_Y = make_batch_dataset(train_dataset1, random_idx)\n",
    "    model1.train()\n",
    "    optimizer1.zero_grad()\n",
    "    batch_pred_Y1 = model1(batch_input_X1, batch_infer_X1)[0]\n",
    "    loss = train_loss10(batch_true_Y, batch_pred_Y1)\n",
    "    loss.backward()\n",
    "    optimizer1.step()\n",
    "    model1.eval()\n",
    "    \n",
    "    random_idx = random.sample(range(len(train_dataset1)), k=batch_size2)\n",
    "    batch_input_X2, batch_infer_X2, batch_true_Y = make_batch_dataset(train_dataset2, random_idx)\n",
    "    model2.train()\n",
    "    optimizer2.zero_grad()\n",
    "    batch_pred_Y2 = model2(batch_input_X2, batch_infer_X2)[0]\n",
    "    loss = train_loss10(batch_true_Y, batch_pred_Y2)\n",
    "    loss.backward()\n",
    "    optimizer2.step()\n",
    "    model2.eval()\n",
    "\n",
    "    temp_val1, _ = validation10(val_true_Y, model1(val_input_X1, val_infer_X1)[0])\n",
    "    temp_val2, _ = validation10(val_true_Y, model2(val_input_X2, val_infer_X2)[0])\n",
    "    if temp_val1 < local_val_loss1:\n",
    "        local_val_loss1 = temp_val1\n",
    "        local_best_model1 = copy.deepcopy(model1)\n",
    "    \n",
    "    if temp_val2 < local_val_loss2:\n",
    "        local_val_loss2 = temp_val2\n",
    "        local_best_model2 = copy.deepcopy(model2)\n",
    "    \n",
    "    local_best_model1.eval()\n",
    "    local_best_model2.eval()\n",
    "    val_loss, val_accuracy, val_incen = validation(val_true_Y, local_best_model1(val_input_X1, val_infer_X1)[0], model2(val_input_X2, val_infer_X2)[0])\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_accuracy = val_accuracy\n",
    "        _, temp_val1 = validation10(val_true_Y, local_best_model1(val_input_X1, val_infer_X1)[0])\n",
    "        _, temp_val2 = validation10(val_true_Y, model2(val_input_X2, val_infer_X2)[0])\n",
    "\n",
    "        print(f\"epoch : {epoch_idx+1}, efficiency: {best_accuracy:.2f}%({temp_val1:.2f}||{temp_val2:.2f}), incentive : {val_incen}, loss : {best_val_loss}\")\n",
    "        best_model1 = copy.deepcopy(local_best_model1)\n",
    "        best_model2 = copy.deepcopy(model2)\n",
    "        torch.save(best_model1, f\"best_model_round1_{train_date}.pt\")\n",
    "        torch.save(best_model2, f\"best_model_round2_{train_date}.pt\")\n",
    "    \n",
    "    val_loss, val_accuracy, val_incen = validation(val_true_Y, model1(val_input_X1, val_infer_X1)[0], local_best_model2(val_input_X2, val_infer_X2)[0])\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_accuracy = val_accuracy\n",
    "        _, temp_val1 = validation10(val_true_Y, model1(val_input_X1, val_infer_X1)[0])\n",
    "        _, temp_val2 = validation10(val_true_Y, local_best_model2(val_input_X2, val_infer_X2)[0])\n",
    "\n",
    "        print(f\"epoch : {epoch_idx+1}, efficiency: {best_accuracy:.2f}%({temp_val1:.2f}||{temp_val2:.2f}), incentive : {val_incen}, loss : {best_val_loss}\")\n",
    "        best_model1 = copy.deepcopy(model1)\n",
    "        best_model2 = copy.deepcopy(local_best_model2)\n",
    "        torch.save(best_model1, f\"best_model_round1_{train_date}.pt\")\n",
    "        torch.save(best_model2, f\"best_model_round2_{train_date}.pt\")\n",
    "    \n",
    "\n",
    "\n",
    "    val_loss, val_accuracy, val_incen = validation(val_true_Y, model1(val_input_X1, val_infer_X1)[0], model2(val_input_X2, val_infer_X2)[0])\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_accuracy = val_accuracy\n",
    "        _, temp_val1 = validation10(val_true_Y, model1(val_input_X1, val_infer_X1)[0])\n",
    "        _, temp_val2 = validation10(val_true_Y, model2(val_input_X2, val_infer_X2)[0])\n",
    "\n",
    "        print(f\"epoch : {epoch_idx+1}, efficiency: {best_accuracy:.2f}%({temp_val1:.2f}||{temp_val2:.2f}), incentive : {val_incen}, loss : {best_val_loss}\")\n",
    "        best_model1 = copy.deepcopy(model1)\n",
    "        best_model2 = copy.deepcopy(model2)\n",
    "        torch.save(best_model1, f\"best_model_round1_{train_date}.pt\")\n",
    "        torch.save(best_model2, f\"best_model_round2_{train_date}.pt\")\n",
    "    else:\n",
    "        val_loss, val_accuracy, val_incen = validation(val_true_Y, best_model1(val_input_X1, val_infer_X1)[0], model2(val_input_X2, val_infer_X2)[0])\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_accuracy = val_accuracy\n",
    "            _, temp_val1 = validation10(val_true_Y, best_model1(val_input_X1, val_infer_X1)[0])\n",
    "            _, temp_val2 = validation10(val_true_Y, model2(val_input_X2, val_infer_X2)[0])\n",
    "\n",
    "            print(f\"epoch : {epoch_idx+1}, efficiency: {best_accuracy:.2f}%({temp_val1:.2f}||{temp_val2:.2f}), incentive : {val_incen}, loss : {best_val_loss}\")            \n",
    "            best_model2 = copy.deepcopy(model2)\n",
    "            torch.save(best_model2, f\"best_model_round2_{train_date}.pt\")\n",
    "        else:\n",
    "            val_loss, val_accuracy, val_incen = validation(val_true_Y, model1(val_input_X1, val_infer_X1)[0], best_model2(val_input_X2, val_infer_X2)[0])\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_accuracy = val_accuracy \n",
    "                _, temp_val1 = validation10(val_true_Y, model1(val_input_X1, val_infer_X1))\n",
    "                _, temp_val2 = validation10(val_true_Y, best_model2(val_input_X2, val_infer_X2))\n",
    "\n",
    "                print(f\"epoch : {epoch_idx+1}, efficiency: {best_accuracy:.2f}%({temp_val1:.2f}||{temp_val2:.2f}), incentive : {val_incen}, loss : {best_val_loss}\")\n",
    "                best_model1 = copy.deepcopy(model1)\n",
    "                torch.save(best_model1, f\"best_model_round1_{train_date}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
